# -*- coding: utf-8 -*-
"""RNN-direction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15lc7pbSX_5l14nEh5__4GT82ASxZr_7i
"""
#Import
import numpy as np
import pandas as pd
import tensorflow as tf 
from sklearn.model_selection import train_test_split

# Import datasets & set datasets

X_data = list()
Y_data = list()
X_test = list()
Y_test = list()
path = './Labeled data/Speed/'

def split(dataset, result, dx, dy):
    for row in dataset:
        rows = list()
        for column in row:
            rows.append(column.split())
        rowdata = [list(map(int, i)) for i in rows]
        dx.append(rowdata[0])
        dy.append(result)


def get(n_datasets, speed, result, dx, dy):
    filename = input('Please type the name of file : ')
    for i in range(1, n_datasets):
        name = path + filename + '/' + speed + '/' + filename + str(i) + '.csv'
        file = pd.read_csv(name)
        dataset = file.values.tolist()
        split(dataset, result, dx, dy)

def load(x, y, result):
  if str(result) == '[1, 0, 0]':
      speed = 'run'
  elif str(result) == '[0, 1, 0]':
      speed = 'walk'
  n_datasets = int(input('Please type the number of datasets : '))
  get(n_datasets, speed, result, x, y)

def load2(x, y, result):
  n_datasets = int(input('Please type the number of datasets : '))
  filename = input('Please type the name of files : ')
  path = './Labeled data/nobody/'
  for i in range(1, n_datasets):
      file= pd.read_csv(path + filename + '/' + filename + str(i) + '.csv')
      dataset = file.values.tolist()
      for row in dataset:
        rows =list()
        for column in row:
          rows.append(column.split())
        rowdata = [list(map(int,i)) for i in rows]
        x.append(rowdata[0])
        y.append(result)




n = int(input("Please type the number of days: "))

for i in range(n):
    load(X_data, Y_data, [1, 0, 0])  # Run
    load(X_data, Y_data, [0, 1, 0])  # Walk
for i in range(2):   
  load2(X_data, Y_data, [0, 0, 1])  # No Exist

X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.3)
print(np.array(X_data).shape)
print(np.array(Y_data).shape)
print(np.array(X_test).shape)
print(np.array(Y_test).shape)

learning_rate = 0.02
total_epochs = 2000
log_path='./Logs/DNN_Speed_1'
save_path = './Model//DNN_Speed_1/DNN'
global_step = tf.Variable(0, trainable=False, name='global_step')

X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

with tf.name_scope('weight'):
  W1 = tf.Variable(tf.random_uniform([100, 100], -1., 1.), name='W1')
  W2 = tf.Variable(tf.random_uniform([100, 50], -1., 1.), name='W2')
  W3 = tf.Variable(tf.random_uniform([50, 25], -1., 1.), name='W3')
  W4 = tf.Variable(tf.random_uniform([25, 3], -1., 1.), name='W4')
  b1 = tf.Variable(tf.zeros([100]), name='b1')
  b2 = tf.Variable(tf.zeros([50]), name='b2')
  b3 = tf.Variable(tf.zeros([25]), name='b3')
  b4 = tf.Variable(tf.zeros([3]), name='b4')
with tf.name_scope('layer1'):
  L1 = tf.add(tf.matmul(X, W1), b1)
  L1 = tf.nn.relu(L1)
with tf.name_scope('layer2'):
  L2 = tf.add(tf.matmul(L1, W2), b2)
  L2 = tf.nn.relu(L2)
with tf.name_scope('layer3'):
  L3 = tf.add(tf.matmul(L2, W3), b3)
  L3 = tf.nn.relu(L3)
with tf.name_scope('layer4'):
  model = tf.add(tf.matmul(L3, W4), b4, name='model')
with tf.name_scope('cost'):
  # Cross-Entropy
  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))
  cost_summ = tf.summary.scalar('cost', cost)
## Training

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
train_op = optimizer.minimize(cost, global_step=global_step, name='op')

with tf.name_scope('accuracy'):
  is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))
  accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
  tf.summary.scalar('accuracy', accuracy)

with tf.Session() as sess:
    merged = tf.summary.merge_all()
    writer = tf.summary.FileWriter(log_path, sess.graph)
    sess = tf.Session()
    saver = tf.train.Saver(tf.global_variables())
    sess.run(tf.global_variables_initializer())

    # make input and target to train
    batch_xs = X_data
    batch_ys = Y_data

    for step in range(total_epochs):
      summary, _, loss =  sess.run([merged, train_op, cost],
                          feed_dict={X: batch_xs, Y: batch_ys})
      writer.add_summary(summary, step)

      print('Step: %d, ' % sess.run(global_step),
            'Cost: %.3f' % sess.run(cost, feed_dict={X: X_data, Y: Y_data}))

saver.save(sess, save_path, global_step=global_step)

prediction = tf.argmax(model, axis=1)
target = tf.argmax(Y, axis=1)
print('예측값: ', sess.run(prediction, feed_dict={X: X_data}))
print('실제값: ', sess.run(target, feed_dict={Y: Y_data}))

# Print the Accuracy

is_correct = tf.equal(prediction, target)
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: X_data, Y: Y_data}))
is_correct = tf.equal(prediction, target)
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: X_test, Y: Y_test}))