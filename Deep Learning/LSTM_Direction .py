# -*- coding: utf-8 -*-
"""RNN-direction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15lc7pbSX_5l14nEh5__4GT82ASxZr_7i
"""
#Import
import numpy as np
import pandas as pd
import tensorflow as tf 
from sklearn.model_selection import train_test_split

# Import datasets & set datasets

X_data = list()
Y_data = list()
X_test = list()
Y_test = list()
path = './Labeled data/Rename_Direction/'

def split(dataset, result, dx, dy):
    for row in dataset:
        rows = list()
        for column in row:
            rows.append(column.split())
        rowdata = [list(map(int, i)) for i in rows]
        dx.append(rowdata[0])
        dy.append(result)


def get(n_datasets, direction, result, dx, dy):
    filename = input('Please type the name of file : ')
    for i in range(1, n_datasets):
        name = path + filename + '/' + direction + '/' + filename + '_' + direction + '_' + str(i) + '.csv'
        file = pd.read_csv(name)
        dataset = file.values.tolist()
        split(dataset, result, dx, dy)

def load(x, y, result):
  if str(result) == '[1, 0, 0]':
      direction = 'right'
  elif str(result) == '[0, 1, 0]':
      direction = 'left'
  n_datasets = int(input('Please type the number of datasets : '))
  get(n_datasets, direction, result, x, y)

def load2(x, y, result):
  n_datasets = int(input('Please type the number of datasets : '))
  filename = input('Please type the name of files : ')
  path = './Labeled data/nobody/'
  for i in range(1, n_datasets):
      file= pd.read_csv(path + filename + '/' + filename + str(i) + '.csv')
      dataset = file.values.tolist()
      for row in dataset:
        rows =list()
        for column in row:
          rows.append(column.split())
        rowdata = [list(map(int,i)) for i in rows]
        x.append(rowdata[0])
        y.append(result)




n = int(input("Please type the number of days: "))

for i in range(n):
    load(X_data, Y_data, [1, 0, 0])  # Right
    load(X_data, Y_data, [0, 1, 0])  # Left
for i in range(2):   
  load2(X_data, Y_data, [0, 0, 1])  # No Exist

X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.3)
print(np.array(X_data).shape)
print(np.array(Y_data).shape)
print(np.array(X_test).shape)
print(np.array(Y_test).shape)

learning_rate = 0.0001
total_epoch = 2000
batch_size = 1

n_input = 100
n_hidden = 10
n_class = 3

log_path='./Logs/RNN_Direction_1'
save_path = './Model/RNN_Direction_1'

tf.reset_default_graph()
# Set X, Y as place holder
X = tf.placeholder(tf.float32, [None, batch_size, n_input], name='X')
Y = tf.placeholder(tf.float32, [None, n_class], name='Y')
dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')

with tf.name_scope('weight'):
  W = tf.Variable(tf.random_uniform([n_hidden, n_class]), name='W')
  b = tf.Variable(tf.zeros([n_class]), name='b')
  
  W_hist = tf.summary.histogram('weight', W)
  b_hist = tf.summary.histogram('bias', b)

with tf.name_scope('RNN'):
#  cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)
  cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)
#  cell = tf.nn.rnn_cell.GRUCell(n_hidden)

with tf.name_scope('Dropout'):
  # To avoid overfitting
  cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_rate)
  cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, name='cell2_LSTM')
  cell2 = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_rate)
  cell3 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, name='cell3_LSTM')\
  cell3 = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_rate)


  # Make combination of cells 
  multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell, cell2, cell3])
  multi_cell = tf.nn.rnn_cell.DropoutWrapper(multi_cell, output_keep_prob=dropout_rate)

with tf.name_scope('output'):
  # Make  Deep RNN
  outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)
  outputs = tf.transpose(outputs, [1, 0, 2])
  outputs = outputs[-1]

  model = tf.add(tf.matmul(outputs, W), b, name='model')
#  model = tf.nn.relu(model)
  model_hist = tf.summary.histogram('model', model)
with tf.name_scope('cost'):
  # Cross-Entropy_v2
  Y_pred = tf.contrib.layers.fully_connected(model, 3, activation_fn=tf.nn.sigmoid)
  cost = tf.losses.mean_squared_error(Y, Y_pred)
  cost_summ = tf.summary.scalar('cost', cost)

optimizer = tf.train.AdamOptimizer(learning_rate, name="optimizer").minimize(cost)

with tf.name_scope('accuracy'):
  # Print the Accuracy

  is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))
  accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
  acc_summ = tf.summary.scalar('accuracy', accuracy)

with tf.Session() as sess:
  
  merged = tf.summary.merge_all() 
  writer = tf.summary.FileWriter(log_path, sess.graph)
  
  # Training 
  sess = tf.Session()
  saver = tf.train.Saver()
  sess.run(tf.global_variables_initializer())

  # make input and target to train
  batch_xs = np.reshape(X_data, (-1, batch_size, n_input))
  batch_ys = Y_data 
 
  for epoch in range(total_epoch): 
     
    summary, _, loss =  sess.run([merged, optimizer, cost],
                      feed_dict={X: batch_xs, Y: batch_ys, dropout_rate})
    writer.add_summary(summary, epoch)
    
    if epoch % 10 == 0:
      print('Epoch:', '%04d' % (epoch + 1),
       'Avg. cost =', '{:.3f}'.format(loss))

  print('최적화 완료!')  



saver.save(sess, save_path, total_epoch)

#test_batch_size = len(X_data)   
test_xs = np.reshape(X_test, (-1, batch_size, n_input))
test_ys = Y_test
print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: batch_xs, Y: batch_ys, dropout_rate: 1}))
print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: test_xs, Y: test_ys, dropout_rate: 1}))

