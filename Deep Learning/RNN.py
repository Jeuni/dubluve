# -*- coding: utf-8 -*-
"""RNN-relu

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QhVo06JyZDooqWCP_miiMxGu0RRkf6KT
"""

#Import
import numpy as np
import pandas as pd
import tensorflow as tf 
from sklearn import datasets, linear_model
from sklearn.model_selection import train_test_split

#Import datasets & set datasets

X_data = list()
Y_data = list()
def load1(x, y, result):
  n_datasets = int(input('Please type the number of datasets : '))
  filename = input('Please type the name of files : ')
  path = './Labeled Data/Labeled_data/'
  for i in range(1, n_datasets):
    name = path + filename + '/' + filename + str(i) + '.csv'
    file= pd.read_csv(name)
    dataset = file.values.tolist()
    for row in dataset:
      rows =list()
      for column in row:
        rows.append(column.split())
      rowdata = [list(map(int,i)) for i in rows]
      x.append(rowdata[0])
      y.append(result)
          
def load2(x, y, result):
  n_datasets = int(input('Please type the number of datasets : '))
  filename = input('Please type the name of files : ')
  path = './Labeled Data/nobody/'
  for i in range(1, n_datasets):
      file= pd.read_csv(path + filename + '/' + filename + str(i) + '.csv')
      dataset = file.values.tolist()
      for row in dataset:
        rows =list()
        for column in row:
          rows.append(column.split())
        rowdata = [list(map(int,i)) for i in rows]
        x.append(rowdata[0])
        y.append(result)

n = int(input("Please type the number of days: "))

for i in range(n):
  load1(X_data, Y_data, [1, 0]) # Exist

for i in range(2):
  load2(X_data, Y_data, [0, 1])



X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.3)

print(np.array(X_data).shape)
print(np.array(Y_data).shape)
print(np.array(X_test).shape)
print(np.array(Y_test).shape)

learning_rate = 0.0002
total_epoch = 2000

n_input = 100
batch_size = 1 
n_hidden = 84
n_class = 2

log_path='./Logs/RNN_PIR1_8'
save_path = './Model/RNN_PIR1_8/RNN'

tf.reset_default_graph()
# Set X, Y as place holder
X = tf.placeholder(tf.float32, [None, 1, n_input], name='X')
Y = tf.placeholder(tf.float32, [None, n_class], name='Y')
dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')

with tf.name_scope('weight'):
  W = tf.Variable(tf.random_uniform([n_hidden, n_class]), name='W')
  b = tf.Variable(tf.zeros([n_class]), name='b')
  
  W_hist = tf.summary.histogram('weight', W)
  b_hist = tf.summary.histogram('bias', b)

with tf.name_scope('RNN'):
#  cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)
  cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)
#  cell = tf.nn.rnn_cell.GRUCell(n_hidden)
# To avoid overfitting
  cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_rate)
  cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, name='cell2_LSTM')
  cell2 = tf.nn.rnn_cell.DropoutWrapper(cell2, output_keep_prob=dropout_rate)
  cell3 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, name='cell3_LSTM')
  cell3 = tf.nn.rnn_cell.DropoutWrapper(cell3, output_keep_prob=dropout_rate)
  cell4 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, name='cell4_LSTM')
  cell4 = tf.nn.rnn_cell.DropoutWrapper(cell4, output_keep_prob=dropout_rate)
  cell5 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, name='cell5_LSTM')
  cell5 = tf.nn.rnn_cell.DropoutWrapper(cell5, output_keep_prob=dropout_rate)

  # Make combination of cells 
  multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell, cell2, cell3])
  multi_cell = tf.nn.rnn_cell.DropoutWrapper(multi_cell, output_keep_prob=dropout_rate)
  
  # Make  Deep RNN
  outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)
  outputs = tf.transpose(outputs, [1, 0, 2])
  outputs = outputs[-1]

  model = tf.add(tf.matmul(outputs, W), b)

#  model_hist = tf.summary.histogram('model', model)
with tf.name_scope('cost'):
  Y_pred = tf.contrib.layers.fully_connected(model, 2, activation_fn=tf.nn.relu)
  cost = tf.losses.mean_squared_error(Y, Y_pred)
  cost_summ = tf.summary.scalar('cost', cost)

with tf.name_scope('optimizer'):
  optimizer = tf.train.AdamOptimizer(learning_rate, name = "optimizer").minimize(cost)

with tf.name_scope('accuracy'):
  # Print the Accuracy
  is_correct = tf.equal(tf.cast(tf.round(Y_pred), tf.float32), Y)
  accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
  tf.summary.scalar('accuracy', accuracy)

with tf.Session() as sess:
  
  merged = tf.summary.merge_all() 
  writer = tf.summary.FileWriter(log_path, sess.graph)
  
  
  sess = tf.Session()
  saver = tf.train.Saver()
  sess.run(tf.global_variables_initializer())

  # make input and target to train
  batch_xs = np.reshape(X_data, (-1, batch_size, n_input))
  batch_ys = Y_data 
  test_xs = np.reshape(X_test, (-1, batch_size, n_input))
  test_ys = Y_test
 
  for epoch in range(total_epoch): 
    summary, _, loss =  sess.run([merged, optimizer, cost], 
                      feed_dict={X: batch_xs, Y: batch_ys, dropout_rate: 0.8})
    writer.add_summary(summary, epoch)
    
    if epoch % 1 == 0:
      print('Epoch:', '%04d' % (epoch + 1),
       'Avg. cost =', '{:.3f}'.format(loss))

  print('최적화 완료!')  



saver.save(sess, save_path, total_epoch)


print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: batch_xs, Y: batch_ys, dropout_rate: 1}))
print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: test_xs, Y: test_ys, dropout_rate: 1}))


"""# **Tensorboard 사용하기**"""
